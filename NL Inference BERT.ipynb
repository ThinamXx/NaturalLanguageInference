{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NL Inference BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d0CN3HareET"
      },
      "source": [
        "### **INITIALIZATION:**\n",
        "- I use these three lines of code on top of my each notebooks because it will help to prevent any problems while reloading the same project. And the third line of code helps to make visualization within the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XdKgcHarBG3"
      },
      "source": [
        "#@ INITIALIZATION:\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcBfheAxrtXV"
      },
      "source": [
        "**DOWNLOADING LIBRARIES AND DEPENDENCIES:**\n",
        "- I have downloaded all the libraries and dependencies required for the project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNchS7vCrqFz"
      },
      "source": [
        "#@ DOWNLOADING THE LIBRARIES AND DEPENDENCIES:\n",
        "# !pip install -U d2l\n",
        "from d2l import torch as d2l\n",
        "\n",
        "import os, json, multiprocessing\n",
        "import torch     \n",
        "from torch import nn                                \n",
        "from torch.nn import functional as F"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luiZQ932sYwN"
      },
      "source": [
        "### **LOADING PRETRAINED BERT:**\n",
        "- The original BERT Model has hundreds of millions of parameters. Here, I have installed two versions of pretrained BERT: BERT Base is the original BERT Model that requires a lot computational resources to fine tune while BERT Small is a small version to facilitate demonstration. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm8UvLs3sBmb"
      },
      "source": [
        "#@ INSTALLING BERT MODEL: \n",
        "d2l.DATA_HUB[\"bert.base\"] = (d2l.DATA_URL + 'bert.base.torch.zip',\n",
        "                             '225d66f04cae318b841a13d32af3acc165f253ac')          # Getting BERT Base Model. \n",
        "d2l.DATA_HUB[\"bert.small\"] = (d2l.DATA_URL + 'bert.small.torch.zip',\n",
        "                              'c72329e68a732bef0452e4b96a1c341c8910f81f')         # Getting BERT Small Model. "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdhSGefUvDes"
      },
      "source": [
        "- The pretrained BERT Model contains a vocab json file that defines the vocabulary set and a pretrained params file of the pretrained parameters. I will define a function to load pretrained BERT parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2UOSouLuyGw"
      },
      "source": [
        "#@ LOADING PRETRAINED BERT MODEL PARAMETERS: \n",
        "def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens, \n",
        "                          num_heads, num_layers, dropout, max_len, devices):             # Loading Pretrained Model. \n",
        "  data_dir = d2l.download_extract(pretrained_model)                                      # Downloading and Extracting Model. \n",
        "  vocab = d2l.Vocab()                                                                    # Initializing Vocabulary. \n",
        "  vocab.idx_to_token = json.load(open(os.path.join(data_dir, \"vocab.json\")))             # Initializing BERT Vocabulary. \n",
        "  vocab.token_to_idx = {token: idx for idx, token in \n",
        "                        enumerate(vocab.idx_to_token)}\n",
        "  bert = d2l.BERTModel(len(vocab), num_hiddens, norm_shape=[256], \n",
        "                       ffn_num_input=256, ffn_num_hiddens=ffn_num_hiddens, \n",
        "                       num_heads=4, num_layers=2, dropout=0.2, max_len=max_len, \n",
        "                       key_size=256, query_size=256, value_size=256, \n",
        "                       hid_in_features=256,mlm_in_features=256,nsp_in_features=256)      # Initializing BERT Model. \n",
        "  bert.load_state_dict(torch.load(os.path.join(data_dir, \"pretrained.params\")))          # Loading Pretrained Model Parameters. \n",
        "  return bert, vocab"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfjtTjGjGJmK"
      },
      "source": [
        "#@ LOADING PRETRAINED BERT MODEL: \n",
        "devices = d2l.try_all_gpus()                                                              # Initialization. \n",
        "bert, vocab = load_pretrained_model(\"bert.small\", num_hiddens=256, ffn_num_hiddens=512, \n",
        "                                    num_heads=4, num_layers=2, dropout=0.1, max_len=512, \n",
        "                                    devices=devices)                                      # Implementation of Function. "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnEBf_D2E9sp"
      },
      "source": [
        "### **DATASET FOR FINE TUNING BERT:**\n",
        "- I will define a customized dataset where each example consist of premise and hypothesis to form a pair of text sequence and is packed into one BERT input sequence. The segment ids are used to distinguish the premise and the hypothesis in a BERT input sequence. I will define a maximum length of BERT input sequence where the last token of longer input text pair keeps getting removed until maximum length is met. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNc7RM1TJOzT"
      },
      "source": [
        "#@ DATASET FOR FINE TUNING BERT MODEL: \n",
        "class SNLIBERTDataset(torch.utils.data.Dataset):                             # Initializing Dataset Class. \n",
        "  def __init__(self, dataset, max_len, vocab=None):                          # Constructor Function. \n",
        "    all_premise_hypothesis_tokens = [[p_tokens, h_tokens] for p_tokens, \n",
        "                                     h_tokens in zip(*[d2l.tokenize(\n",
        "                                         [s.lower() for s in sentences])\n",
        "                                     for sentences in dataset[:2]])]         # Initializing Premise and Hypothesis Tokens. \n",
        "    self.labels = torch.tensor(dataset[2])                                   # Initializing Labels. \n",
        "    self.vocab = vocab                                                       # Initializing Vocabulary. \n",
        "    self.max_len = max_len                                                   # Initializing Maximum Length of Inputs. \n",
        "    (self.all_token_ids,self.all_segments,self.valid_lens)=self._preprocess(\n",
        "        all_premise_hypothesis_tokens)                                       # Initializing Preprocess Function. \n",
        "    print(\"read\", str(len(self.all_token_ids)), \"examples\")                  # Inspecting the Data. \n",
        "\n",
        "  def _preprocess(self, all_premise_hypothesis_tokens):                      # Preprocess Function. \n",
        "    pool = multiprocessing.Pool(4)                                           # Using 4 Worker Processess. \n",
        "    out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)           # Mapping Tokens. \n",
        "    all_token_ids = [token_ids for token_ids, segments, valid_len in out]    # Getting Token IDs.\n",
        "    all_segments = [segments for token_ids, segments, valid_len in out]      # Getting Segments. \n",
        "    valid_lens = [valid_len for token_ids, segments, valid_len in out]       # Getting Valid Length. \n",
        "    return (torch.tensor(all_token_ids, dtype=torch.long), \n",
        "            torch.tensor(all_segments, dtype=torch.long), \n",
        "            torch.tensor(valid_lens))\n",
        "  \n",
        "  def _mp_worker(self, premise_hypothesis_tokens):\n",
        "    p_tokens, h_tokens = premise_hypothesis_tokens                           # Initializing Tokens.  \n",
        "    self._truncate_pair_of_tokens(p_tokens, h_tokens)                        # Initializing Truncating Function. \n",
        "    tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)       # Getting Tokens and Segments. \n",
        "    token_ids = self.vocab[tokens] + [self.vocab[\"<pad>\"]] * (self.max_len- \n",
        "                                                              len(tokens))   # Getting Tokens IDs. \n",
        "    segments = segments + [0] * (self.max_len - len(segments))               # Getting Segments. \n",
        "    valid_len = len(tokens)                                                  # Number of Tokens. \n",
        "    return token_ids, segments, valid_len\n",
        "  \n",
        "  def _truncate_pair_of_tokens(self, p_tokens, h_tokens):                    # Truncating Tokens. \n",
        "    while len(p_tokens) + len(h_tokens) > self.max_len - 3:                  # Inspecting Length of Tokens. \n",
        "      if len(p_tokens) > len(h_tokens):\n",
        "        p_tokens.pop()                                                       # Removing Last Token. \n",
        "      else: \n",
        "        h_tokens.pop()                                                       # Removing Last Token. \n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return (self.all_token_ids[idx], self.all_segments[idx], \n",
        "            self.valid_lens[idx], self.labels[idx])\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.all_token_ids)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr3pQUKtRnbi",
        "outputId": "dc4fc807-b19c-4fd8-d80c-67aeec0cce80"
      },
      "source": [
        "#@ GENERATING TRAINING AND TESTING EXAMPLES: \n",
        "batch_size, max_len, num_workers = 512, 128, 2                               # Initializing Parameters. \n",
        "data_dir = d2l.download_extract(\"SNLI\")                                      # Downloading and Extracting. \n",
        "train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)   # Getting Training Set. \n",
        "test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)   # Getting Test Set. \n",
        "train_iter = torch.utils.data.DataLoader(train_set, batch_size,shuffle=True, \n",
        "                                         num_workers=num_workers)            # Initializing Training Iterations. \n",
        "test_iter = torch.utils.data.DataLoader(test_set, batch_size,shuffle=False, \n",
        "                                        num_workers=num_workers)             # Initializing Test Iterations. "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "read 549367 examples\n",
            "read 9824 examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7BqTGS2TL5l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}