{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NL Inference BERT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d0CN3HareET"
      },
      "source": [
        "### **INITIALIZATION:**\n",
        "- I use these three lines of code on top of my each notebooks because it will help to prevent any problems while reloading the same project. And the third line of code helps to make visualization within the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XdKgcHarBG3"
      },
      "source": [
        "#@ INITIALIZATION:\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcBfheAxrtXV"
      },
      "source": [
        "**DOWNLOADING LIBRARIES AND DEPENDENCIES:**\n",
        "- I have downloaded all the libraries and dependencies required for the project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNchS7vCrqFz"
      },
      "source": [
        "#@ DOWNLOADING THE LIBRARIES AND DEPENDENCIES:\n",
        "# !pip install -U d2l\n",
        "from d2l import torch as d2l\n",
        "\n",
        "import os, json, multiprocessing\n",
        "import torch     \n",
        "from torch import nn                                \n",
        "from torch.nn import functional as F"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luiZQ932sYwN"
      },
      "source": [
        "### **LOADING PRETRAINED BERT:**\n",
        "- The original BERT Model has hundreds of millions of parameters. Here, I have installed two versions of pretrained BERT: BERT Base is the original BERT Model that requires a lot computational resources to fine tune while BERT Small is a small version to facilitate demonstration. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm8UvLs3sBmb"
      },
      "source": [
        "#@ INSTALLING BERT MODEL: \n",
        "d2l.DATA_HUB[\"bert.base\"] = (d2l.DATA_URL + 'bert.base.torch.zip',\n",
        "                             '225d66f04cae318b841a13d32af3acc165f253ac')          # Getting BERT Base Model. \n",
        "d2l.DATA_HUB[\"bert.small\"] = (d2l.DATA_URL + 'bert.small.torch.zip',\n",
        "                              'c72329e68a732bef0452e4b96a1c341c8910f81f')         # Getting BERT Small Model. "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdhSGefUvDes"
      },
      "source": [
        "- The pretrained BERT Model contains a vocab json file that defines the vocabulary set and a pretrained params file of the pretrained parameters. I will define a function to load pretrained BERT parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2UOSouLuyGw"
      },
      "source": [
        "#@ LOADING PRETRAINED BERT MODEL PARAMETERS: \n",
        "def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens, \n",
        "                          num_heads, num_layers, dropout, max_len, devices):             # Loading Pretrained Model. \n",
        "  data_dir = d2l.download_extract(pretrained_model)                                      # Downloading and Extracting Model. \n",
        "  vocab = d2l.Vocab()                                                                    # Initializing Vocabulary. \n",
        "  vocab.idx_to_token = json.load(open(os.path.join(data_dir, \"vocab.json\")))             # Initializing BERT Vocabulary. \n",
        "  vocab.token_to_idx = {token: idx for idx, token in \n",
        "                        enumerate(vocab.idx_to_token)}\n",
        "  bert = d2l.BERTModel(len(vocab), num_hiddens, norm_shape=[256], \n",
        "                       ffn_num_input=256, ffn_num_hiddens=ffn_num_hiddens, \n",
        "                       num_heads=4, num_layers=2, dropout=0.2, max_len=max_len, \n",
        "                       key_size=256, query_size=256, value_size=256, \n",
        "                       hid_in_features=256,mlm_in_features=256,nsp_in_features=256)      # Initializing BERT Model. \n",
        "  bert.load_state_dict(torch.load(os.path.join(data_dir, \"pretrained.params\")))          # Loading Pretrained Model Parameters. \n",
        "  return bert, vocab"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfjtTjGjGJmK"
      },
      "source": [
        "#@ LOADING PRETRAINED BERT MODEL: \n",
        "devices = d2l.try_all_gpus()                                                              # Initialization. \n",
        "bert, vocab = load_pretrained_model(\"bert.small\", num_hiddens=256, ffn_num_hiddens=512, \n",
        "                                    num_heads=4, num_layers=2, dropout=0.1, max_len=512, \n",
        "                                    devices=devices)                                      # Implementation of Function. "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2gkKiV0Gwbx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}