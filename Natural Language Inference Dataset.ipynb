{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural Language Inference Dataset.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNHgcfbD6EnA"
      },
      "source": [
        "**INITIALIZATION:**\n",
        "- I use these three lines of code on top of my each notebooks because it will help to prevent any problems while reloading the same project. And the third line of code helps to make visualization within the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4gIqNju5yX0"
      },
      "source": [
        "#@ INITIALIZATION:\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vThfPbYl6NEu"
      },
      "source": [
        "**DOWNLOADING LIBRARIES AND DEPENDENCIES:**\n",
        "- I have downloaded all the libraries and dependencies required for the project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdwDrLFY6KBd"
      },
      "source": [
        "#@ DOWNLOADING THE LIBRARIES AND DEPENDENCIES:\n",
        "# !pip install -U d2l\n",
        "from d2l import torch as d2l\n",
        "\n",
        "import os, re\n",
        "import torch     \n",
        "from torch import nn                                \n",
        "from IPython import display"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9rzHdgy6jKU"
      },
      "source": [
        "**GETTING THE DATASET:**\n",
        "- I have used google colab for this notebook so the process of downloading and reading the data might be different in other platforms. I will use **Stanford Natural Language Inference Corpus** for this notebook. The SNLI Corpus is a collection of over 500000 labeled english pairs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4MSubTR6aSH"
      },
      "source": [
        "#@ GETTING THE DATASET: \n",
        "d2l.DATA_HUB[\"SNLI\"] = ('https://nlp.stanford.edu/projects/snli/snli_1.0.zip',\n",
        "                        '9fcde07509c7e87ec61c640c1b2753d9041758e4')               # Reading the Dataset. \n",
        "data_dir = d2l.download_extract(\"SNLI\")                                           # Extracting the Dataset. "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PccaJGBe8DHz"
      },
      "source": [
        "**READING THE DATASET:**\n",
        "- I will define a function to only extract part of the dataset and then return list of premises, hypothesis and their labels. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkBkqEP48AbJ"
      },
      "source": [
        "#@ READING THE DATASET: \n",
        "def read_snli(data_dir, is_train):                                # Reading Dataset into Premises, Hypothesis and Labels. \n",
        "  def extract_text(s):                                            # Removing unwanted Texts. \n",
        "    s = re.sub(\"\\\\(\", \"\", s)                                      # Removing Information. \n",
        "    s = re.sub(\"\\\\)\", \"\", s)                                      # Removing Information. \n",
        "    s = re.sub(\"\\\\s{2,}\", \" \", s)                                 # Replacing Whitespaces with Space. \n",
        "    return s.strip()\n",
        "  \n",
        "  label_set = {\"entailment\": 0, \"contradiction\": 1, \n",
        "               \"neutral\": 2}                                      # Initializing Labels. \n",
        "  file_name = os.path.join(data_dir, \"snli_1.0_train.txt\" if \\\n",
        "                           is_train else \"snli_1.0_test.txt\")\n",
        "  with open(file_name, \"r\") as f: \n",
        "    rows = [row.split(\"\\t\") for row in f.readlines()[1:]]\n",
        "  premises = [extract_text(row[1]) for row in rows if row[0] in \n",
        "              label_set]                                          # Initializing Premises. \n",
        "  hypothesis = [extract_text(row[2]) for row in rows if row[0] \\\n",
        "                in label_set]                                     # Initializing Hypothesis. \n",
        "  labels = [label_set[row[0]] for row in rows if row[0] in \n",
        "            label_set]                                            # Initializing Labels. \n",
        "  return premises, hypothesis, labels"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG_5xUO_-gDG",
        "outputId": "c29ba3be-54f0-4ca7-9710-9e18a9b049bf"
      },
      "source": [
        "#@ IMPLEMENTATION: \n",
        "train_data = read_snli(data_dir, is_train=True)                   # Implementation of Function. \n",
        "for x0, x1, y in zip(train_data[0][:3], train_data[1][:3], \n",
        "                     train_data[2][:3]):\n",
        "  print(\"premise:\", x0)                                           # Inspecting Premises. \n",
        "  print(\"hypothesis:\", x1)                                        # Inspecting Hypothesis. \n",
        "  print(\"label:\", y)                                              # Inspecting Labels. "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "premise: A person on a horse jumps over a broken down airplane .\n",
            "hypothesis: A person is training his horse for a competition .\n",
            "label: 2\n",
            "premise: A person on a horse jumps over a broken down airplane .\n",
            "hypothesis: A person is at a diner , ordering an omelette .\n",
            "label: 1\n",
            "premise: A person on a horse jumps over a broken down airplane .\n",
            "hypothesis: A person is outdoors , on a horse .\n",
            "label: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIf5nej3Bclu",
        "outputId": "5a475c55-e27e-4bc9-b8e3-12fd0d404878"
      },
      "source": [
        "#@ READING THE DATASET: \n",
        "test_data = read_snli(data_dir, is_train=False)                   # Implementation of Function. \n",
        "for data in [train_data, test_data]:\n",
        "  print([[row for row in data[2]].count(i) for i in range(3)])    # Inspecting the Data. "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[183416, 183187, 182764]\n",
            "[3368, 3237, 3219]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVzVurUdKMb8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}