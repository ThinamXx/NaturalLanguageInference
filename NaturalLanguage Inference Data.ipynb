{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural Language Inference Dataset.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNHgcfbD6EnA"
      },
      "source": [
        "**INITIALIZATION:**\n",
        "- I use these three lines of code on top of my each notebooks because it will help to prevent any problems while reloading the same project. And the third line of code helps to make visualization within the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4gIqNju5yX0"
      },
      "source": [
        "#@ INITIALIZATION:\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vThfPbYl6NEu"
      },
      "source": [
        "**DOWNLOADING LIBRARIES AND DEPENDENCIES:**\n",
        "- I have downloaded all the libraries and dependencies required for the project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdwDrLFY6KBd"
      },
      "source": [
        "#@ DOWNLOADING THE LIBRARIES AND DEPENDENCIES:\n",
        "# !pip install -U d2l\n",
        "from d2l import torch as d2l\n",
        "\n",
        "import os, re\n",
        "import torch     \n",
        "from torch import nn                                \n",
        "from IPython import display"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9rzHdgy6jKU"
      },
      "source": [
        "**GETTING THE DATASET:**\n",
        "- I have used google colab for this notebook so the process of downloading and reading the data might be different in other platforms. I will use **Stanford Natural Language Inference Corpus** for this notebook. The SNLI Corpus is a collection of over 500000 labeled english pairs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4MSubTR6aSH"
      },
      "source": [
        "#@ GETTING THE DATASET: \n",
        "d2l.DATA_HUB[\"SNLI\"] = ('https://nlp.stanford.edu/projects/snli/snli_1.0.zip',\n",
        "                        '9fcde07509c7e87ec61c640c1b2753d9041758e4')               # Reading the Dataset. \n",
        "data_dir = d2l.download_extract(\"SNLI\")                                           # Extracting the Dataset. "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PccaJGBe8DHz"
      },
      "source": [
        "**READING THE DATASET:**\n",
        "- I will define a function to only extract part of the dataset and then return list of premises, hypothesis and their labels. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkBkqEP48AbJ"
      },
      "source": [
        "#@ READING THE DATASET: \n",
        "def read_snli(data_dir, is_train):                                # Reading Dataset into Premises, Hypothesis and Labels. \n",
        "  def extract_text(s):                                            # Removing unwanted Texts. \n",
        "    s = re.sub(\"\\\\(\", \"\", s)                                      # Removing Information. \n",
        "    s = re.sub(\"\\\\)\", \"\", s)                                      # Removing Information. \n",
        "    s = re.sub(\"\\\\s{2,}\", \" \", s)                                 # Replacing Whitespaces with Space. \n",
        "    return s.strip()\n",
        "  \n",
        "  label_set = {\"entailment\": 0, \"contradiction\": 1, \n",
        "               \"neutral\": 2}                                      # Initializing Labels. \n",
        "  file_name = os.path.join(data_dir, \"snli_1.0_train.txt\" if \\\n",
        "                           is_train else \"snli_1.0_test.txt\")\n",
        "  with open(file_name, \"r\") as f: \n",
        "    rows = [row.split(\"\\t\") for row in f.readlines()[1:]]\n",
        "  premises = [extract_text(row[1]) for row in rows if row[0] in \n",
        "              label_set]                                          # Initializing Premises. \n",
        "  hypothesis = [extract_text(row[2]) for row in rows if row[0] \\\n",
        "                in label_set]                                     # Initializing Hypothesis. \n",
        "  labels = [label_set[row[0]] for row in rows if row[0] in \n",
        "            label_set]                                            # Initializing Labels. \n",
        "  return premises, hypothesis, labels"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG_5xUO_-gDG",
        "outputId": "677a1f89-755e-4024-d15f-c77197b46f92"
      },
      "source": [
        "#@ IMPLEMENTATION: \n",
        "train_data = read_snli(data_dir, is_train=True)                   # Implementation of Function. \n",
        "for x0, x1, y in zip(train_data[0][:3], train_data[1][:3], \n",
        "                     train_data[2][:3]):\n",
        "  print(\"premise:\", x0)                                           # Inspecting Premises. \n",
        "  print(\"hypothesis:\", x1)                                        # Inspecting Hypothesis. \n",
        "  print(\"label:\", y)                                              # Inspecting Labels. "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "premise: A person on a horse jumps over a broken down airplane .\n",
            "hypothesis: A person is training his horse for a competition .\n",
            "label: 2\n",
            "premise: A person on a horse jumps over a broken down airplane .\n",
            "hypothesis: A person is at a diner , ordering an omelette .\n",
            "label: 1\n",
            "premise: A person on a horse jumps over a broken down airplane .\n",
            "hypothesis: A person is outdoors , on a horse .\n",
            "label: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIf5nej3Bclu",
        "outputId": "903631e5-6fcc-4dbe-bef9-844bed843d5b"
      },
      "source": [
        "#@ READING THE DATASET: \n",
        "test_data = read_snli(data_dir, is_train=False)                   # Implementation of Function. \n",
        "for data in [train_data, test_data]:\n",
        "  print([[row for row in data[2]].count(i) for i in range(3)])    # Inspecting the Data. "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[183416, 183187, 182764]\n",
            "[3368, 3237, 3219]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Rn23a-5TJlg"
      },
      "source": [
        "**LOADING THE DATASET:**\n",
        "- I will define a class for loading the SNLI Dataset. The num steps argument in the class constructor specifies the length of a text sequence so that each minibatch of sequences will have the same shape. The token sequences which are longer than num steps are trimmed while special tokkens are appended to shorter sequences. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVzVurUdKMb8"
      },
      "source": [
        "#@ LOADING THE DATASET: \n",
        "class SNLIDataset(torch.utils.data.Dataset):                     # Loading SNLI Dataset. \n",
        "  def __init__(self, dataset, num_steps, vocab=None):            # Initializing Constructor Function. \n",
        "    self.num_steps = num_steps                                   # Initialization. \n",
        "    all_premise_tokens = d2l.tokenize(dataset[0])                # Initializing Tokenization. \n",
        "    all_hypothesis_tokens = d2l.tokenize(dataset[1])             # Initializing Tokenization. \n",
        "    if vocab is None: \n",
        "      self.vocab = d2l.Vocab(all_premise_tokens + \\\n",
        "                             all_hypothesis_tokens, min_freq=5, \n",
        "                             reserved_tokens=[\"<pad>\"])          # Initializing Vocabulary of Tokens. \n",
        "    else: \n",
        "      self.vocab = vocab\n",
        "    self.premises = self._pad(all_premise_tokens)                # Implementation of Padding and Truncation. \n",
        "    self.hypotheses = self._pad(all_hypothesis_tokens)           # Implementation of Padding and Truncation. \n",
        "    self.labels = torch.tensor(dataset[2])                       # Initializing Labels. \n",
        "    print(\"read \" + str(len(self.premises)) + \" examples\")\n",
        "  \n",
        "  def _pad(self, lines):\n",
        "    return torch.tensor([d2l.truncate_pad(self.vocab[line], \n",
        "                                          self.num_steps, \n",
        "                                          self.vocab[\"<pad>\"])\\\n",
        "                         for line in lines])\n",
        "    \n",
        "  def __getitem__(self, idx):                                    # Accessing Premise, Hypothesis and Labels. \n",
        "    return (self.premises[idx], self.hypotheses[idx]), \\\n",
        "            self.labels[idx]\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.premises)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6H3zvEVYAbb"
      },
      "source": [
        "#@ LOADING THE DATASET: \n",
        "def load_data_snli(batch_size, num_steps=50):                       # Initializing Data Iterations and Vocabulary. \n",
        "  num_workers = d2l.get_dataloader_workers()                        # Initialization. \n",
        "  data_dir = d2l.download_extract(\"SNLI\")                           # Extracting the Dataset. \n",
        "  train_data = read_snli(data_dir, True)                            # Initializing Training Dataset. \n",
        "  test_data = read_snli(data_dir, False)                            # Initializing Test Dataset. \n",
        "  train_set = SNLIDataset(train_data, num_steps)                    # Initializing Training Set. \n",
        "  test_set = SNLIDataset(test_data, num_steps, \n",
        "                         train_set.vocab)                           # Initializing Test Set. \n",
        "  train_iter = torch.utils.data.DataLoader(train_set, batch_size, \n",
        "                                           shuffle=True, \n",
        "                                           num_workers=2)           # Initializing Training Iterations. \n",
        "  test_iter = torch.utils.data.DataLoader(test_set, batch_size, \n",
        "                                          shuffle=False, \n",
        "                                          num_workers=2)            # Initializing Test Iterations. \n",
        "  return train_iter, test_iter, train_set.vocab                                        "
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fFzU8AnajVw",
        "outputId": "bef55dbf-4a96-42f9-f9ce-352e73a66c1d"
      },
      "source": [
        "#@ IMPLEMENTATION:\n",
        "train_iter, test_iter, vocab = load_data_snli(128, 50)              # Implementation of Function. \n",
        "print(len(vocab))                                                   # Inspecting Vocabulary. "
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "read 549367 examples\n",
            "read 9824 examples\n",
            "18678\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaxcnCwTazvM",
        "outputId": "bea18541-ffc8-410a-b7a7-c317a67865f1"
      },
      "source": [
        "#@ IMPLEMENTATION: \n",
        "for X, Y in train_iter: \n",
        "  print(X[0].shape)\n",
        "  print(X[1].shape)\n",
        "  print(Y.shape)\n",
        "  break"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 50])\n",
            "torch.Size([128, 50])\n",
            "torch.Size([128])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}