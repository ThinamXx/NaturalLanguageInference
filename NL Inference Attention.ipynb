{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NL Inference Attention.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY6V5Xsdgc7s"
      },
      "source": [
        "### **INITIALIZATION:**\n",
        "- I use these three lines of code on top of my each notebooks because it will help to prevent any problems while reloading the same project. And the third line of code helps to make visualization within the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74Q0iC_rfUzZ"
      },
      "source": [
        "#@ INITIALIZATION:\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6MPuUIegpNZ"
      },
      "source": [
        "**DOWNLOADING LIBRARIES AND DEPENDENCIES:**\n",
        "- I have downloaded all the libraries and dependencies required for the project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc1_B1N8gmvu"
      },
      "source": [
        "#@ DOWNLOADING THE LIBRARIES AND DEPENDENCIES:\n",
        "# !pip install -U d2l\n",
        "from d2l import torch as d2l\n",
        "\n",
        "import os\n",
        "import torch     \n",
        "from torch import nn                                \n",
        "from torch.nn import functional as F"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW6qDs1NhHXD"
      },
      "source": [
        "**GETTING THE DATASET:**\n",
        "- I have used google colab for this notebook so the process of downloading and reading the data might be different in other platforms. I will use Stanford Natural Language Inference Corpus for this notebook. The SNLI Corpus is a collection of over 500000 labeled english pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qLWE9SLg-fl",
        "outputId": "b4cb11a0-4ea3-4985-b2a8-3cdf96c20e8d"
      },
      "source": [
        "#@ GETTING THE DATASET: \n",
        "batch_size, num_steps = 256, 50                                          # Initializing Parameters. \n",
        "train_iter, test_iter, vocab = d2l.load_data_snli(batch_size, num_steps) # Initializing Data Iterations. "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "read 549367 examples\n",
            "read 9824 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrHHLML0i5se"
      },
      "source": [
        "### **DECOMPOSABLE ATTENTION MODEL:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEx_YkR_rx5G"
      },
      "source": [
        "**ATTENDING CLASS:**\n",
        "- I will align words in one text sequnce to each word in other sequence. I will implement soft alignment using attention mechanism. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d23U_ZEHiDKH"
      },
      "source": [
        "#@ IMPLEMENTING MULTILAYER PERCEPTRON: \n",
        "def mlp(num_inputs, num_hiddens, flatten):                 # Function for MLP.\n",
        "  net = []                                                 # Initializing Lists.\n",
        "  net.append(nn.Dropout(0.2))                              # Initializing Dropout Layer. \n",
        "  net.append(nn.Linear(num_inputs, num_hiddens))           # Initializing Linear Layer. \n",
        "  net.append(nn.ReLU())                                    # Initializing RELU Activation. \n",
        "  if flatten:\n",
        "    net.append(nn.Flatten(start_dim=1))                    # Initializing Flatten Layer. \n",
        "  net.append(nn.Dropout(0.2))                              # Initializing Dropout Layer. \n",
        "  net.append(nn.Linear(num_hiddens, num_hiddens))          # Initializing Linear Layer. \n",
        "  net.append(nn.ReLU())                                    # Initializing RELU Activation. \n",
        "  if flatten:\n",
        "    net.append(nn.Flatten(start_dim=1))                    # Initializing Flatten Layer. \n",
        "  return nn.Sequential(*net)                               # Initializing Sequential API. "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkxKSdIVoFcs"
      },
      "source": [
        "- I will define the `Attend Class` to compute the soft alignment of the hypotheses `beta` with input premises and soft alignment of premises `alpha` with input hypotheses. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgC9eqLBlmac"
      },
      "source": [
        "#@ IMPLEMENTATION OF ATTEND CLASS: \n",
        "class Attend(nn.Module):                                            # Initializing Attend Class. \n",
        "  def __init__(self, num_inputs, num_hiddens, **kwargs):            # Initializing Constructor Function. \n",
        "    super(Attend, self).__init__(**kwargs)\n",
        "    self.f = mlp(num_inputs, num_hiddens, flatten=False)            # Initialization of MLP. \n",
        "  \n",
        "  def forward(self, A, B):                                          # Forward Propagation Function. \n",
        "    f_A = self.f(A)                                                 # Implementation of MLP. \n",
        "    f_B = self.f(B)                                                 # Implementation of MLP. \n",
        "    e = torch.bmm(f_A, f_B.permute(0, 2, 1))                        # Implementation of Matrix Multiplication. \n",
        "    beta = torch.bmm(F.softmax(e, dim=-1), B)                       # Implementation of Softmax. \n",
        "    alpha = torch.bmm(F.softmax(e.permute(0, 2, 1), dim=-1), A)     # Implementation of Softmax. \n",
        "    return beta, alpha"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocQgVMx3lzAk"
      },
      "source": [
        "**COMPARING CLASS:**\n",
        "- I will compare a word in one sequence with the other sequence that is softly aligned with the word. In soft alignment all the words from one sequence with different attention weights will be compared with a word in other sequence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rej-J8OnsWQm"
      },
      "source": [
        "#@ INITIALIZING COMPARING CLASS: \n",
        "class Compare(nn.Module):                                       # Initializing Compare. \n",
        "  def __init__(self, num_inputs, num_hiddens, **kwargs):        # Initializing Constructor Function. \n",
        "    super(Compare, self).__init__(**kwargs)\n",
        "    self.g = mlp(num_inputs, num_hiddens, flatten=False)        # Implementation of MLP Function. \n",
        "  \n",
        "  def forward(self, A, B, beta, alpha):                         # Forward Propagation Function. \n",
        "    V_A = self.g(torch.cat([A, beta], dim=2))                   # Initializing Concatenation. \n",
        "    V_B = self.g(torch.cat([B, alpha], dim=2))                  # Initializing Concatenation. \n",
        "    return V_A, V_B"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGt5crOipJla"
      },
      "source": [
        "**AGGREGATING CLASS:**\n",
        "- I will aggregate the two sets of comparison vectors to infer the logical relationship. I will feed the concatenation of both summarization results into MLP function to obtain the classification result of the logical relationship. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpNdINQoouQk"
      },
      "source": [
        "#@ INITIALIZING AGGREGATING CLASS: \n",
        "class Aggregate(nn.Module):                                               # Initializing Aggregating. \n",
        "  def __init__(self, num_inputs, num_hiddens, num_outputs, **kwargs):     # Initializing Constructor Function. \n",
        "    super(Aggregate, self).__init__(**kwargs)\n",
        "    self.h = mlp(num_inputs, num_hiddens, flatten=True)                   # Initializing MLP Classifier. \n",
        "    self.linear = nn.Linear(num_hiddens, num_outputs)                     # Initializing Output Layer. \n",
        "  \n",
        "  def forward(self, V_A, V_B):                                            # Forward Propagation Function. \n",
        "    V_A = V_A.sum(dim=1)                                                  # Getting sum of Comparison Vectors. \n",
        "    V_B = V_B.sum(dim=1)                                                  # Getting sum of Comparison Vectors. \n",
        "    Y_hat = self.Linear(self.h(torch.cat([V_A, V_B], dim=1)))             # Implementation of MLP and Linear Layer. \n",
        "    return Y_hat"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CxyYQ8mr7Kg"
      },
      "source": [
        "- By using the attending, comparing and aggregating steps together, I will define the decomposable attention model to jointly train these three steps. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JffTsiJmrW3B"
      },
      "source": [
        "#@ IMPLEMENTATION OF DECOMPOSABLE ATTENTION MODEL: \n",
        "class DecomposableAttention(nn.Module):                                         # Initializing Decomposable Attention Model. \n",
        "  def __init__(self, vocab, embed_size, num_hiddens, num_inputs_attend=100, \n",
        "               num_inputs_compare=200, num_inputs_agg=400, **kwargs):           # Initializing Constructor Function. \n",
        "    super(DecomposableAttention, self).__init__(**kwargs)         \n",
        "    self.embedding = nn.Embedding(len(vocab), embed_size)                       # Initializing Embedding Layer. \n",
        "    self.attend = Attend(num_inputs_attend, num_hiddens)                        # Initializing Attending Class. \n",
        "    self.compare = Compare(num_inputs_compare, num_hiddens)                     # Initializing Comparing Class. \n",
        "    self.aggregate = Aggregate(num_inputs_agg, num_hiddens, num_outputs=3)      # Initializing Aggregating Class. \n",
        "  \n",
        "  def forward(self, X):                                                         # Forward Propagation Function. \n",
        "    premises, hypotheses = X \n",
        "    A = self.embedding(premises)                                                # Implementation of Embedding. \n",
        "    B = self.embedding(hypotheses)                                              # Implementation of Embedding. \n",
        "    beta, alpha = self.attend(A, B)                                             # Implementation of Attending. \n",
        "    V_A, V_B = self.compare(A, B, beta, alpha)                                  # Implementation of Comparing. \n",
        "    Y_hat = self.aggregate(V_A, V_B)                                            # Implementation of Aggregating. \n",
        "    return Y_hat"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwMx-fISvJSG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}