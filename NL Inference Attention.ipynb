{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NL Inference Attention.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY6V5Xsdgc7s"
      },
      "source": [
        "### **INITIALIZATION:**\n",
        "- I use these three lines of code on top of my each notebooks because it will help to prevent any problems while reloading the same project. And the third line of code helps to make visualization within the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74Q0iC_rfUzZ"
      },
      "source": [
        "#@ INITIALIZATION:\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6MPuUIegpNZ"
      },
      "source": [
        "**DOWNLOADING LIBRARIES AND DEPENDENCIES:**\n",
        "- I have downloaded all the libraries and dependencies required for the project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc1_B1N8gmvu"
      },
      "source": [
        "#@ DOWNLOADING THE LIBRARIES AND DEPENDENCIES:\n",
        "# !pip install -U d2l\n",
        "from d2l import torch as d2l\n",
        "\n",
        "import os\n",
        "import torch     \n",
        "from torch import nn                                \n",
        "from torch.nn import functional as F"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW6qDs1NhHXD"
      },
      "source": [
        "**GETTING THE DATASET:**\n",
        "- I have used google colab for this notebook so the process of downloading and reading the data might be different in other platforms. I will use Stanford Natural Language Inference Corpus for this notebook. The SNLI Corpus is a collection of over 500000 labeled english pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qLWE9SLg-fl",
        "outputId": "58342b71-307a-4f13-caad-6c05702dfbd6"
      },
      "source": [
        "#@ GETTING THE DATASET: \n",
        "batch_size, num_steps = 256, 50                                          # Initializing Parameters. \n",
        "train_iter, test_iter, vocab = d2l.load_data_snli(batch_size, num_steps) # Initializing Data Iterations. "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "read 549367 examples\n",
            "read 9824 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrHHLML0i5se"
      },
      "source": [
        "### **ATTENDING CLASS:**\n",
        "- I will align words in one text sequnce to each word in other sequence. I will implement soft alignment using attention mechanism. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d23U_ZEHiDKH"
      },
      "source": [
        "#@ IMPLEMENTING MULTILAYER PERCEPTRON: \n",
        "def mlp(num_inputs, num_hiddens, flatten):                 # Function for MLP.\n",
        "  net = []                                                 # Initializing Lists.\n",
        "  net.append(nn.Dropout(0.2))                              # Initializing Dropout Layer. \n",
        "  net.append(nn.Linear(num_inputs, num_hiddens))           # Initializing Linear Layer. \n",
        "  net.append(nn.ReLU())                                    # Initializing RELU Activation. \n",
        "  if flatten:\n",
        "    net.append(nn.Flatten(start_dim=1))                    # Initializing Flatten Layer. \n",
        "  net.append(nn.Dropout(0.2))                              # Initializing Dropout Layer. \n",
        "  net.append(nn.Linear(num_hiddens, num_hiddens))          # Initializing Linear Layer. \n",
        "  net.append(nn.ReLU())                                    # Initializing RELU Activation. \n",
        "  if flatten:\n",
        "    net.append(nn.Flatten(start_dim=1))                    # Initializing Flatten Layer. \n",
        "  return nn.Sequential(*net)                               # Initializing Sequential API. "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkxKSdIVoFcs"
      },
      "source": [
        "- I will define the `Attend Class` to compute the soft alignment of the hypotheses `beta` with input premises and soft alignment of premises `alpha` with input hypotheses. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgC9eqLBlmac"
      },
      "source": [
        "#@ IMPLEMENTATION OF ATTEND CLASS: \n",
        "class Attend(nn.Module):                                            # Initializing Attend Class. \n",
        "  def __init__(self, num_inputs, num_hiddens, **kwargs):            # Initializing Constructor Function. \n",
        "    super(Attend, self).__init__(**kwargs)\n",
        "    self.f = mlp(num_inputs, num_hiddens, flatten=False)            # Initialization of MLP. \n",
        "  \n",
        "  def forward(self, A, B):                                          # Forward Propagation Function. \n",
        "    f_A = self.f(A)                                                 # Implementation of MLP. \n",
        "    f_B = self.f(B)                                                 # Implementation of MLP. \n",
        "    e = torch.bmm(f_A, f_B.permute(0, 2, 1))                        # Implementation of Matrix Multiplication. \n",
        "    beta = torch.bmm(F.softmax(e, dim=-1), B)                       # Implementation of Softmax. \n",
        "    alpha = torch.bmm(F.softmax(e.permute(0, 2, 1), dim=-1), A)     # Implementation of Softmax. \n",
        "    return beta, alpha"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rej-J8OnsWQm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}